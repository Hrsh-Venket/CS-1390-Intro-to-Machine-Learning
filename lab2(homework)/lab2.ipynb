{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ee178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chance of Admit '].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f33b2d",
   "metadata": {},
   "source": [
    "We have to convert `Chance of Admit` to a binary decision `Admit`. For this we are using the median as the threshold. This will give nearly equal number of Admits and Rejects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb789ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chance of Admit '].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c260235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Admit'] = df['Chance of Admit ']>0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc03cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['GRE Score'],df['Chance of Admit '])\n",
    "plt.scatter(df['GRE Score'],df['Admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ec0d3e4",
   "metadata": {},
   "source": [
    "In the simple example below I have used only one column (GRE Score) as a feature, and Chance of Admit as the target output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef06416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['GRE Score'].to_numpy()[:,np.newaxis]\n",
    "# X = df['GRE Score']\n",
    "Y = df['Chance of Admit ']\n",
    "Y = Y.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot shows the nice linear regression fit between GRE score and Chance of admit\n",
    "\n",
    "plt.plot(X,Y,'.')\n",
    "plt.plot(X,lr.predict(X),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using linear regression now do logistic regression on target output classes of Admit\n",
    "Y = df['Admit']\n",
    "Y = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d949fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the Logistic Regression prediction (along with ground truth)\n",
    "LR.fit(X,Y)\n",
    "plt.plot(X,Y,'.')\n",
    "plt.plot(X,LR.predict(X),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b7a6ed",
   "metadata": {},
   "source": [
    "### Q1\n",
    "``` \n",
    "A. Use 4 features from above to set up your data matrix X. These 4 features should in your opinion best predict Admit decision (4 pts)\n",
    "B. Split X, Y into train, val, test (2 pts) \n",
    "C. Scale and Augment X appropriately (4 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc142d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1A\n",
    "\n",
    "X = df #['GRE Score', 'Research', 'University Rating', 'LOR', 'CGPA'].to_numpy()\n",
    "X = X.filter(['GRE Score', 'Research', 'University Rating', 'LOR', 'CGPA'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B\n",
    "\n",
    "def train_val_test_split(df, Y):\n",
    "    p = np.random.permutation(len(Y))\n",
    "    tr = np.floor(len(Y)*0.7).astype('int')\n",
    "    te = np.floor(len(Y)*0.8).astype('int')\n",
    "    df_train = df.iloc[p[:tr], :]\n",
    "    Y_train = Y[p[:tr]]\n",
    "    df_val = df.iloc[p[tr+1:te], :]\n",
    "    Y_val = Y[p[tr+1:te]]\n",
    "    df_test = df.iloc[p[te+1:], :]\n",
    "    Y_test = Y[p[te+1:]]\n",
    "    return df_train, Y_train, df_val, Y_val, df_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = train_val_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_aug_train =  np.c_[X_train,np.ones(len(Y_train))]\n",
    "X_aug_val =  np.c_[X_val,np.ones(len(Y_val))]\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "LR.fit(X_aug_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd269f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cross_entropy_loss(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    epsilon = 1e-5\n",
    "    cost = (1/m)*(((~y).T @ np.log(h + epsilon))-((~(1-y)).T @ np.log(1-h + epsilon)))\n",
    "    return cost\n",
    "\n",
    "# def compute_cross_entropy_loss(X, y, theta):\n",
    "#     m = len(y)\n",
    "#     h = sigmoid(X @ theta)\n",
    "#     epsilon = 1e-5\n",
    "#     cost = (1/m)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
    "#     return cost\n",
    "\n",
    "def predict(X, params):\n",
    "    return np.round(sigmoid(X @ params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0de8a2e",
   "metadata": {},
   "source": [
    "### Q2\n",
    "``` Report cross entropy loss for a random prediction of Y_val and for predictions from LR.predict(X_val) (5 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e13a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(X,zeros=True):\n",
    "    n = X.shape[1]\n",
    "    if zeros:\n",
    "        theta = np.zeros((n,1))\n",
    "    else:\n",
    "        theta = np.random.rand(n,1)-0.5\n",
    "        theta[-1] = 0\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is batch gradient descent that updates using all training samples\n",
    "def update_weights( X, Y,  theta ) :\n",
    "             \n",
    "        #Y_pred = predict(X, theta)  \n",
    "        # calculate gradients  \n",
    "        m = X.shape[0]\n",
    "        dtheta = - ( 2 * ( X.T ).dot( Y - sigmoid(np.matmul(X,theta)) )  ) / m\n",
    "        cost_history = compute_cross_entropy_loss(X, y, params)  \n",
    "        return dtheta,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = init(X_aug_train)\n",
    "X_aug_val = X_aug_val.values\n",
    "\n",
    "# figure out why this isn't working\n",
    "# Compute cross-entropy loss for random prediction\n",
    "random_pred = np.random.rand(len(Y_val))\n",
    "random_loss = compute_cross_entropy_loss(X_aug_val, random_pred, theta)\n",
    "print(f\"Cross-entropy loss for random prediction: {random_loss}\")\n",
    "\n",
    "# Compute cross-entropy loss for LR.predict(X_val)\n",
    "lr_pred = LR.predict(X_aug_val)\n",
    "lr_pred = lr_pred.reshape(-1, 1)\n",
    "lr_loss = compute_cross_entropy_loss(X_aug_val, lr_pred, theta)\n",
    "print(f\"Cross-entropy loss for LR.predict(X_val): {lr_loss}\")\n",
    "\n",
    "# # Compute cross-entropy loss for random prediction\n",
    "# random_pred = np.random.rand(len(Y_val))\n",
    "# random_loss = compute_cross_entropy_loss(X_val, Y_val, random_pred)\n",
    "# random_loss\n",
    "\n",
    "# # Compute cross-entropy loss for LR.predict(X_val)\n",
    "# lr_pred = LR.predict(X_val)\n",
    "# lr_pred = LR.predict(X_val).reshape(-1, 1)\n",
    "\n",
    "# lr_loss = compute_cross_entropy_loss(X_val, Y_val, lr_pred)\n",
    "# print(f\"Cross-entropy loss for LR.predict(X_val): {lr_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6e3c5",
   "metadata": {},
   "source": [
    "### Q3 \n",
    "``` Implement an iterative method that at each iterations selects a random theta and if this theta improves cross_entropy_loss keeps the theta, else discards the theta. plot the cross_entropy loss history (over iterations for X_val) with this method. (10 pts)```\n",
    "### Bonus\n",
    "``` Implement an iterative method that at each iterations gets a random *dtheta* and if  theta+learning_rate*dtheta improves cross_entropy_loss it updates  theta with dtheta, else discards dtheta. plot the cross_entropy loss history (over iterations for X_val) with this method. (10 pts)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c90e2",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "``` From Lab1 get the gradient descent  -- update over entire training sample, update over a  sample, update over a batch of sample -- that worked best for you. Also play with learning rate to get to the best cross_entropy_loss.  plot the cross_entropy loss history (over iterations for X_val) with this method. (20 pts)```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e1d087",
   "metadata": {},
   "source": [
    "There is an alternate to cross_entropy_loss for binary classification. It is known as hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037e9fa",
   "metadata": {},
   "source": [
    "$l(x_i) =  max(0,1-y_i\\theta x_i)$ ```with the assumption that y_i = {+1, -1}```\n",
    "\n",
    "``` if y_i and \\theta x_i have same sign and |\\theta x_i| is larger than one, loss will be zero. That is prediction matches label and prediction has magnitude greater than one there is no loss. If prediction and label have opposite sign, loss will be greater than zero -- incorrect prediction there is a loss. There is also a loss if magnitude of prediction is less than zero even if they have the same sign. Hinge loss wants correct and incorrect classification to have a margin of atleast one. ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b24ffbf",
   "metadata": {},
   "source": [
    "### Q5\n",
    "``` \n",
    "Implement Hinge loss and use random search method in Q3 to reduce loss and find a better theta. plto the hinge loss history (over iterations for X_val) with this method. (10 pts)\n",
    "```\n",
    "### Bonus\n",
    "```\n",
    "Implement SGD update rule for hinge loss by first find derivative of hinge loss over theta. Use SGD to optimize hinge loss. plot the hinge loss history (over iterations for X_val) with this method. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5098cb",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "```  In this problem you will create your own target function f (probability in this case) and data set D to see how Logistic Regression works. For simplicity, we will take f to be a 0/1 probability so y is a deterministic function of x. Take n = 2 so you can visualize the problem, and let X = [−1, 1]×[−1, 1] with uniform probability of picking each x ∈ X . Choose a line in the plane as the boundary between f(x) = 1 (where y has to be +1) and f(x) = 0 (where y has to be −1) by taking two random, uniformly distributed points from X and taking the line passing through them as the boundary between y = ±1. Pick m = 100 training points at random from X, and evaluate the outputs y_m for each of these points x_m.```\n",
    "\n",
    "```\n",
    "Run Logistic Regression with Stochastic Gradient Descent to find g, and estimate E_out (the cross entropy error) by generating a sufficiently large, separate set of points to evaluate the error. Repeat the experiment for 100 runs with different targets and take the average. Initialize the weight vector of Logistic Regression to all zeros in each run. Stop the algorithm when ∥w(t−1) − w(t)∥ < 0.01, where w(t) denotes the weight vector at the end of epoch t. An epoch is a full pass through the N data points (use a random permutation of 1, 2, · · · , N to present the data points to the algorithm within each epoch, and use different permutations for different epochs). Use a learning rate of 0.01.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64c4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
