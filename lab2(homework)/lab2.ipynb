{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ee178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chance of Admit '].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f33b2d",
   "metadata": {},
   "source": [
    "We have to convert `Chance of Admit` to a binary decision `Admit`. For this we are using the median as the threshold. This will give nearly equal number of Admits and Rejects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb789ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chance of Admit '].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c260235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Admit'] = df['Chance of Admit ']>0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc03cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['GRE Score'],df['Chance of Admit '])\n",
    "plt.scatter(df['GRE Score'],df['Admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ec0d3e4",
   "metadata": {},
   "source": [
    "In the simple example below I have used only one column (GRE Score) as a feature, and Chance of Admit as the target output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef06416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['GRE Score'].to_numpy()[:,np.newaxis]\n",
    "Y = df['Chance of Admit ']\n",
    "Y = Y.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot shows the nice linear regression fit between GRE score and Chance of admit\n",
    "\n",
    "plt.plot(X,Y,'.')\n",
    "plt.plot(X,lr.predict(X),'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using linear regression now do logistic regression on target output classes of Admit\n",
    "Y = df['Admit']\n",
    "Y = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d949fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the Logistic Regression prediction (along with ground truth)\n",
    "LR.fit(X,Y)\n",
    "plt.plot(X,Y,'.')\n",
    "plt.plot(X,LR.predict(X),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b7a6ed",
   "metadata": {},
   "source": [
    "### Q1\n",
    "``` \n",
    "A. Use 4 features from above to set up your data matrix X. These 4 features should in your opinion best predict Admit decision (4 pts)\n",
    "B. Split X, Y into train, val, test (2 pts) \n",
    "C. Scale and Augment X appropriately (4 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df['GRE Score'].to_numpy()[:,np.newaxis] # red\n",
    "lr.fit(X1,Y)\n",
    "plt.plot(X1,Y,'.')\n",
    "plt.plot(X1,lr.predict(X1),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df['TOEFL Score'].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X2,Y)\n",
    "plt.plot(X2,Y,'.')\n",
    "plt.plot(X2,lr.predict(X2),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2eb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = df['University Rating'].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X3,Y)\n",
    "plt.plot(X3,Y,'.')\n",
    "plt.plot(X3,lr.predict(X3),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee796c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = df['SOP'].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X4,Y)\n",
    "plt.plot(X4,Y,'.')\n",
    "plt.plot(X4,lr.predict(X4),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = df['LOR '].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X5,Y)\n",
    "plt.plot(X5,Y,'.')\n",
    "plt.plot(X5,lr.predict(X5),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = df['CGPA'].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X6,Y)\n",
    "plt.plot(X6,Y,'.')\n",
    "plt.plot(X6,lr.predict(X6),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X7 = df['Research'].to_numpy()[:,np.newaxis]\n",
    "lr.fit(X7,Y)\n",
    "plt.plot(X7,Y,'.')\n",
    "plt.plot(X7,lr.predict(X7),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc142d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1A\n",
    "\n",
    "X = df  # based on the above, I see that these 4 features seem to be most strongly \n",
    "        # correlated to chance of admit\n",
    "X = df.filter(['GRE Score', 'TOEFL Score', 'SOP', 'CGPA'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B\n",
    "\n",
    "def train_val_test_split(df, Y):\n",
    "    p = np.random.permutation(len(Y))\n",
    "    tr = np.floor(len(Y)*0.7).astype('int')\n",
    "    te = np.floor(len(Y)*0.8).astype('int')\n",
    "    df_train = df.iloc[p[:tr], :]\n",
    "    Y_train = Y[p[:tr]]\n",
    "    df_val = df.iloc[p[tr+1:te], :]\n",
    "    Y_val = Y[p[tr+1:te]]\n",
    "    df_test = df.iloc[p[te+1:], :]\n",
    "    Y_test = Y[p[te+1:]]\n",
    "    return df_train, Y_train, df_val, Y_val, df_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = train_val_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_aug_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_aug_val = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
    "X_aug_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_aug_train =  np.c_[X_train,np.ones(len(Y_train))]\n",
    "# X_aug_val =  np.c_[X_val,np.ones(len(Y_val))]\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "LR.fit(X_aug_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd269f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def compute_cross_entropy_loss(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    epsilon = 1e-5\n",
    "    cost = (1/m)*(((~y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
    "    return cost\n",
    "\n",
    "# above is corrected function\n",
    "# def compute_cross_entropy_loss(X, y, theta):\n",
    "#     m = len(y)\n",
    "#     h = sigmoid(X @ theta)\n",
    "#     epsilon = 1e-5\n",
    "#     cost = (1/m)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
    "#     return cost\n",
    "\n",
    "def predict(X, params):\n",
    "    return np.round(sigmoid(X @ params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0de8a2e",
   "metadata": {},
   "source": [
    "### Q2\n",
    "``` Report cross entropy loss for a random prediction of Y_val and for predictions from LR.predict(X_val) (5 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e13a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(X,zeros=True):\n",
    "    n = X.shape[1]\n",
    "    if zeros:\n",
    "        theta = np.zeros((n,1))\n",
    "    else:\n",
    "        theta = np.random.rand(n,1)-0.5\n",
    "        theta[-1] = 0\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is batch gradient descent that updates using all training samples\n",
    "def update_weights( X, Y,  theta ) :\n",
    "             \n",
    "        #Y_pred = predict(X, theta)  \n",
    "        # calculate gradients  \n",
    "        m = X.shape[0]\n",
    "        dtheta = - ( 2 * ( X.T ).dot( Y - sigmoid(np.matmul(X,theta)) )  ) / m\n",
    "        cost_history = compute_cross_entropy_loss(X, Y, dtheta)  # changed from y, params\n",
    "        return dtheta,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Part 1\n",
    "np.random.seed(1)  \n",
    "n = len(Y_val)\n",
    "k = np.unique(Y_val).size\n",
    "random_pred = np.random.rand(n, k)\n",
    "random_pred /= random_pred.sum(axis=1, keepdims=True)\n",
    "\n",
    "onehot_Y_val = np.zeros((n, k))\n",
    "onehot_Y_val[np.arange(n), np.clip(Y_val, 0, k-1)] = 1\n",
    "\n",
    "random_ce_loss = -(onehot_Y_val * np.log(random_pred)).sum() / n\n",
    "print(random_ce_loss)\n",
    "\n",
    "# Q2 Part 2\n",
    "theta = init(X_val)\n",
    "dtheta, cost_h = update_weights(X_val, Y_val, theta)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, Y_train)\n",
    "y_pred = LR.predict(X_val)\n",
    "dtheta, cost_h = update_weights(X_val, y_pred, theta)\n",
    "y_rand = np.random.rand(*Y_val.shape)\n",
    "y_rand = y_rand >= 0.5\n",
    "dtheta, cost_h = update_weights(X_val, y_rand, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6e3c5",
   "metadata": {},
   "source": [
    "### Q3 \n",
    "``` Implement an iterative method that at each iterations selects a random theta and if this theta improves cross_entropy_loss keeps the theta, else discards the theta. plot the cross_entropy loss history (over iterations for X_val) with this method. (10 pts)```\n",
    "### Bonus\n",
    "``` Implement an iterative method that at each iterations gets a random *dtheta* and if  theta+learning_rate*dtheta improves cross_entropy_loss it updates  theta with dtheta, else discards dtheta. plot the cross_entropy loss history (over iterations for X_val) with this method. (10 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomise(X_train, y_train, X_val, y_val, max_iter):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    history = []\n",
    "    best_loss = float('inf')\n",
    "    for i in range(max_iter):\n",
    "        new_theta = np.random.rand(n, 1)\n",
    "        new_loss = compute_cross_entropy_loss(X_val, y_val, new_theta)\n",
    "        if abs(new_loss) < abs(best_loss):\n",
    "            best_loss = new_loss\n",
    "            theta = new_theta\n",
    "        history.append(best_loss)\n",
    "    return theta, history\n",
    "\n",
    "best_theta, loss_history = randomise(X_train, Y_train, X_val, Y_val, 1000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c90e2",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "``` From Lab1 get the gradient descent  -- update over entire training sample, update over a  sample, update over a batch of sample -- that worked best for you. Also play with learning rate to get to the best cross_entropy_loss.  plot the cross_entropy loss history (over iterations for X_val) with this method. (20 pts)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent over the entire training sample\n",
    "\n",
    "def gradient_descent(X_train, y_train, X_val, y_val, learning_rate, max_iter):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    history = []\n",
    "    best_loss = float('inf')\n",
    "    for i in range(max_iter):\n",
    "        dtheta = np.random.randn(n, 1)\n",
    "        new_theta = theta + learning_rate * dtheta\n",
    "        new_loss = compute_cross_entropy_loss(X_val, y_val, new_theta)\n",
    "        if abs(new_loss) < abs(best_loss):\n",
    "            best_loss = new_loss\n",
    "            theta = new_theta\n",
    "        history.append(best_loss)\n",
    "    return theta, history\n",
    "\n",
    "learning_rate = 0.001 # the issue with this case is that there is no local minima or maxima.\n",
    "                    # so higher learning rate is always better\n",
    "max_iter = 200\n",
    "best_theta, loss_history = gradient_descent(X_train, Y_train, X_val, Y_val, learning_rate, max_iter)\n",
    "print(loss_history[-1])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "116998e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[524], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# the issue with this case is that there is no local minima or maxima.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# so higher learning rate is always better\u001b[39;00m\n\u001b[1;32m     48\u001b[0m max_iter \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m---> 49\u001b[0m best_theta, loss_history \u001b[39m=\u001b[39m gradient_descent(X_train, Y_train, X_val, Y_val, max_iter, learning_rate, \u001b[39m50\u001b[39;49m)\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(loss_history[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     51\u001b[0m plt\u001b[39m.\u001b[39mplot(loss_history)\n",
      "Cell \u001b[0;32mIn[524], line 35\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X_train, y_train, X_val, y_val, max_iter, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m     y_i \u001b[39m=\u001b[39m y_train[start_idx:end_idx]\n\u001b[1;32m     34\u001b[0m     dtheta \u001b[39m=\u001b[39m update_weights(x_i, y_i, new_theta)\n\u001b[0;32m---> 35\u001b[0m     new_theta \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m dtheta\n\u001b[1;32m     36\u001b[0m new_loss \u001b[39m=\u001b[39m compute_cross_entropy_loss(X_val, y_val, new_theta)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(new_loss) \u001b[39m<\u001b[39m \u001b[39mabs\u001b[39m(best_loss):\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,50)"
     ]
    }
   ],
   "source": [
    "# Run batched gradient descent \n",
    "def init(X,zeros=True):\n",
    "    n = X.shape[1]\n",
    "    if zeros:\n",
    "        theta = np.zeros((n,1))\n",
    "    else:\n",
    "        theta = np.random.rand(n,1)-0.5\n",
    "        theta[-1] = 0\n",
    "    return theta\n",
    "\n",
    "\n",
    "\n",
    "def update_weights( X, Y,  theta ) :\n",
    "        Y_pred = predict(X, theta)  \n",
    "        m = X.shape[0]\n",
    "        dtheta = - ( 2 * ( X.T ).dot( Y - Y_pred )  ) / m\n",
    "        return dtheta\n",
    "\n",
    "def gradient_descent(X_train, y_train, X_val, y_val, max_iter, learning_rate, batch_size):\n",
    "    m, n = X_train.shape\n",
    "    num_batches = m // batch_size\n",
    "    theta = np.zeros((n, 1))\n",
    "    history = []\n",
    "    best_loss = float('inf')\n",
    "    new_theta = init(X_train)\n",
    "    theta = init(X_train)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = (batch + 1) * batch_size\n",
    "            x_i = X_train[start_idx:end_idx, :]\n",
    "            y_i = y_train[start_idx:end_idx]\n",
    "            dtheta = update_weights(x_i, y_i, theta)\n",
    "            new_theta -= learning_rate * dtheta\n",
    "        new_loss = compute_cross_entropy_loss(X_val, y_val, new_theta)\n",
    "        if abs(new_loss) < abs(best_loss):\n",
    "            best_loss = new_loss\n",
    "            history.append(best_loss)\n",
    "            theta = new_theta\n",
    "    return theta, history\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0001 \n",
    "# the issue with this case is that there is no local minima or maxima.\n",
    "# so higher learning rate is always better\n",
    "max_iter = 1000\n",
    "best_theta, loss_history = gradient_descent(X_train, Y_train, X_val, Y_val, max_iter, learning_rate, 50)\n",
    "print(loss_history[-1])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape #(39,4)\n",
    "Y_val.shape #(39,)\n",
    "print(new_theta) #4,50\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e1d087",
   "metadata": {},
   "source": [
    "There is an alternate to cross_entropy_loss for binary classification. It is known as hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037e9fa",
   "metadata": {},
   "source": [
    "$l(x_i) =  max(0,1-y_i\\theta x_i)$ ```with the assumption that y_i = {+1, -1}```\n",
    "\n",
    "``` if y_i and \\theta x_i have same sign and |\\theta x_i| is larger than one, loss will be zero. That is prediction matches label and prediction has magnitude greater than one there is no loss. If prediction and label have opposite sign, loss will be greater than zero -- incorrect prediction there is a loss. There is also a loss if magnitude of prediction is less than zero even if they have the same sign. Hinge loss wants correct and incorrect classification to have a margin of atleast one. ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b24ffbf",
   "metadata": {},
   "source": [
    "### Q5\n",
    "``` \n",
    "Implement Hinge loss and use random search method in Q3 to reduce loss and find a better theta. plto the hinge loss history (over iterations for X_val) with this method. (10 pts)\n",
    "```\n",
    "### Bonus\n",
    "```\n",
    "Implement SGD update rule for hinge loss by first find derivative of hinge loss over theta. Use SGD to optimize hinge loss. plot the hinge loss history (over iterations for X_val) with this method. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = X @ theta\n",
    "    loss = np.maximum(0, 1 - y * h)\n",
    "    return np.mean(loss)\n",
    "\n",
    "def randomise(X_train, y_train, X_val, y_val, max_iter):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    history = []\n",
    "    best_loss = float('inf')\n",
    "    for i in range(max_iter):\n",
    "        new_theta = np.random.rand(n, 1)\n",
    "        new_loss = hinge_loss(X_val, y_val, new_theta)\n",
    "        if abs(new_loss) < abs(best_loss):\n",
    "            best_loss = new_loss\n",
    "            theta = new_theta\n",
    "        history.append(best_loss)\n",
    "    return theta, history\n",
    "\n",
    "max_iter = 1000\n",
    "best_theta, loss_history = randomise(X_train, Y_train, X_val, Y_val, max_iter)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Hinge loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb28e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_val, y_val, learning_rate, max_iter):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    history = []\n",
    "    best_loss = float('inf')\n",
    "    for i in range(max_iter):\n",
    "        dtheta = np.random.randn(n, 1)\n",
    "        new_theta = theta + learning_rate * dtheta\n",
    "        new_loss = hinge_loss(X_val, y_val, new_theta)\n",
    "        if abs(new_loss) < abs(best_loss):\n",
    "            best_loss = new_loss\n",
    "            theta = new_theta\n",
    "        history.append(best_loss)\n",
    "    return theta, history\n",
    "\n",
    "learning_rate = 0.1 # global max is only local max and model is linear, so higher local minimum always peforms better\n",
    "max_iter = 1000\n",
    "best_theta, loss_history = gradient_descent(X_train, Y_train, X_val, Y_val, learning_rate, max_iter)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Hinge loss')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b845e38c",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "```  In this problem you will create your own target function f (probability in this case) and data set D to see how Logistic Regression works. For simplicity, we will take f to be a 0/1 probability so y is a deterministic function of x. Take n = 2 so you can visualize the problem, and let X = [−1, 1]×[−1, 1] with uniform probability of picking each x ∈ X . Choose a line in the plane as the boundary between f(x) = 1 (where y has to be +1) and f(x) = 0 (where y has to be −1) by taking two random, uniformly distributed points from X and taking the line passing through them as the boundary between y = ±1. Pick m = 100 training points at random from X, and evaluate the outputs y_m for each of these points x_m.```\n",
    "\n",
    "```\n",
    "Run Logistic Regression with Stochastic Gradient Descent to find g, and estimate E_out (the cross entropy error) by generating a sufficiently large, separate set of points to evaluate the error. Repeat the experiment for 100 runs with different targets and take the average. Initialize the weight vector of Logistic Regression to all zeros in each run. Stop the algorithm when ∥w(t−1) − w(t)∥ < 0.01, where w(t) denotes the weight vector at the end of epoch t. An epoch is a full pass through the N data points (use a random permutation of 1, 2, · · · , N to present the data points to the algorithm within each epoch, and use different permutations for different epochs). Use a learning rate of 0.01.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa892f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    Target function to classify the points above or below a random line\n",
    "    \"\"\"\n",
    "    # Generate a random line between two points\n",
    "    point1 = (-1, -1)\n",
    "    point2 = (1, 1)\n",
    "    a = (point2[1] - point1[1]) / (point2[0] - point1[0])\n",
    "    b = point1[1] - a * point1[0]\n",
    "    \n",
    "    # Evaluate the output y based on the position of x with respect to the line\n",
    "    if x[1] > a * x[0] + b:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "m = 100\n",
    "X = np.random.uniform(low=-1, high=1, size=(m, 2))\n",
    "\n",
    "# Evaluate the output y for each point in X using the target function f\n",
    "y = np.array([f(x) for x in X])\n",
    "\n",
    "plt.scatter(X[:, 0][y == 1], X[:, 1][y == 1], marker='o', color='b', label='y=+1')\n",
    "plt.scatter(X[:, 0][y == -1], X[:, 1][y == -1], marker='x', color='r', label='y=-1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weight vector\n",
    "w = np.zeros(3)\n",
    "\n",
    "# Define the learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Define the stopping criterion\n",
    "epsilon = 0.01\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Define the number of training points\n",
    "m = 100\n",
    "\n",
    "# Define the number of test points\n",
    "test_size = 10000\n",
    "\n",
    "# Define the number of runs\n",
    "runs = 100\n",
    "\n",
    "# Define the cross-entropy error\n",
    "ce_error = np.zeros(runs)\n",
    "\n",
    "# Repeat for each run\n",
    "for r in range(runs):\n",
    "    # Generate the training data\n",
    "    X_train = np.random.uniform(-1, 1, size=(m, 2))\n",
    "    y_train = np.sign((X_train[:, 1] - X_train[:, 0]) * (np.random.uniform() * 2 - 1))\n",
    "    X_train = np.hstack((np.ones((m, 1)), X_train))\n",
    "    \n",
    "    # Generate the test data\n",
    "    X_test = np.random.uniform(-1, 1, size=(test_size, 2))\n",
    "    y_test = np.sign((X_test[:, 1] - X_test[:, 0]) * (np.random.uniform() * 2 - 1))\n",
    "    X_test = np.hstack((np.ones((test_size, 1)), X_test))\n",
    "    \n",
    "    # Perform stochastic gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        # Randomize the order of the training points\n",
    "        perm = np.random.permutation(m)\n",
    "        X_train = X_train[perm, :]\n",
    "        y_train = y_train[perm]\n",
    "        \n",
    "        # Iterate over the training points\n",
    "        for i in range(m):\n",
    "            # Compute the sigmoid function and its gradient\n",
    "            sig = 1 / (1 + np.exp(-y_train[i] * X_train[i, :].dot(w)))\n",
    "            grad = (-y_train[i] * sig) * X_train[i, :]\n",
    "            \n",
    "            # Update the weight vector\n",
    "            w_new = w - lr * grad\n",
    "            \n",
    "            # Check the stopping criterion\n",
    "            if np.linalg.norm(w_new - w) < epsilon:\n",
    "                w = w_new\n",
    "                break\n",
    "            \n",
    "            w = w_new\n",
    "    \n",
    "    # Compute the cross-entropy error\n",
    "    ce_error[r] = np.mean(np.log(1 + np.exp(-y_test * X_test.dot(w))))\n",
    "    \n",
    "# Compute the average cross-entropy error\n",
    "avg_ce_error = np.mean(ce_error)\n",
    "\n",
    "print(avg_ce_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameter values\n",
    "n = 0.03  # Population growth rate\n",
    "delta = 0.07  # Depreciation rate\n",
    "s = 0.2  # Savings rate\n",
    "A = 1  # Technology level\n",
    "\n",
    "# Production function\n",
    "def Y(K, L):\n",
    "    return A * (K ** 0.5) * (L ** 0.5)\n",
    "\n",
    "# Capital accumulation equation\n",
    "def K_dot(K, L):\n",
    "    return s * Y(K, L) - delta * K\n",
    "\n",
    "# Plotting the Solow model diagram\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Capital grid\n",
    "k_grid = np.linspace(0, 100, 1000)\n",
    "ax.plot(k_grid, Y(k_grid, 1), 'b-', lw=2, alpha=0.8)\n",
    "\n",
    "# Steady state level of capital per worker\n",
    "k_star = ((s * A) / (delta + n)) ** 2\n",
    "y_star = Y(k_star, 1)\n",
    "c_star = (1 - s) * y_star\n",
    "\n",
    "# Initial level of capital per worker\n",
    "k0 = k_star / 2\n",
    "y0 = Y(k0, 1)\n",
    "c0 = (1 - s) * y0\n",
    "\n",
    "# Transition path to new steady state\n",
    "T = 20\n",
    "k_path = np.zeros(T)\n",
    "y_path = np.zeros(T)\n",
    "c_path = np.zeros(T)\n",
    "k_path[0] = k0\n",
    "y_path[0] = y0\n",
    "c_path[0] = c0\n",
    "for t in range(1, T):\n",
    "    k_path[t] = K_dot(k_path[t-1], 1) + k_path[t-1]\n",
    "    y_path[t] = Y(k_path[t], 1)\n",
    "    c_path[t] = (1 - s) * y_path[t]\n",
    "\n",
    "# Plotting the transition path\n",
    "ax.plot(k_path, y_path, 'g-', lw=2, alpha=0.8)\n",
    "\n",
    "# Plotting the initial steady state\n",
    "ax.plot(k0, y0, 'ro', ms=10)\n",
    "ax.annotate('Initial Steady State', xy=(k0, y0), xytext=(k0+10, y0-5), fontsize=12,\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "# Plotting the new steady state\n",
    "ax.plot(k_star, y_star, 'ro', ms=10)\n",
    "ax.annotate('New Steady State', xy=(k_star, y_star), xytext=(k_star-30, y_star+10), fontsize=12,\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "# Setting plot parameters\n",
    "ax.set_xlabel('Capital per Worker')\n",
    "ax.set_ylabel('Output per Worker')\n",
    "ax.set_xlim([0, 100])\n",
    "ax.set_ylim([0, 10])\n",
    "ax.set_title('Solow Model Diagram with Transition to New Steady State')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
