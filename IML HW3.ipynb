{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IML HW3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first question, generating a 50000 * 50000 matrix was effectively impossible with the level of compute available to me. Thereby, I could notice significant difference in speeds of the two PCA functions before going up to this size. Hence, I stopped at a function of size 1000 (samples) * 100 (parameters). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```cpp\n",
    "// due to issues setting up eigen path, run with:\n",
    "// g++ PCA.cpp -o test -I /usr/include/eigen3\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <Eigen/Dense>\n",
    "#include <vector>\n",
    "#include <assert.h>\n",
    "#include <fstream>\n",
    " \n",
    "using Eigen::MatrixXd;\n",
    "\n",
    "\n",
    "using namespace Eigen;\n",
    "\n",
    "template<typename M>\n",
    "M load_csv (const std::string & path) {\n",
    "    std::ifstream indata;\n",
    "    indata.open(path);\n",
    "    std::string line;\n",
    "    std::vector<double> values;\n",
    "    uint rows = 0;\n",
    "    while (std::getline(indata, line)) {\n",
    "        std::stringstream lineStream(line);\n",
    "        std::string cell;\n",
    "        while (std::getline(lineStream, cell, ',')) {\n",
    "            values.push_back(std::stod(cell));\n",
    "        }\n",
    "        ++rows;\n",
    "    }\n",
    "    return Map<const Matrix<typename M::Scalar, M::RowsAtCompileTime, M::ColsAtCompileTime, RowMajor>>(values.data(), rows, values.size()/rows);\n",
    "}\n",
    "\n",
    "\n",
    "Eigen::VectorXd PCA(MatrixXd m){\n",
    "  // time measurement\n",
    "\n",
    "\n",
    "  // Generate Covariance matrix \n",
    "  Eigen::MatrixXd centered = m.rowwise() - m.colwise().mean(); // subtract column means from each row\n",
    "  Eigen::MatrixXd Cov = (1.0/(m.rows()-1)) * centered.adjoint() * centered; // calculate covariance matrix\n",
    "  assert(Cov.rows() == Cov.cols() && \"Covariance matrix is not square\"); // this is a check\n",
    "  // find eigenvalues \n",
    "  Eigen::EigenSolver<Eigen::MatrixXd> solver(Cov);\n",
    "  Eigen::VectorXd eigenvalues = solver.eigenvalues().real();\n",
    "  // since the covariance matrix is always syymetric, are eigenvalues are never complex\n",
    "  // find largest eigenvalue\n",
    "  double largest_eigenvalue = eigenvalues.maxCoeff();\n",
    "  // find eigenvector corresponding to largest eigenvalue\n",
    "  Eigen::MatrixXcd eigenvectors = solver.eigenvectors(); // find all eigenvectors\n",
    "  int largest_eigenvalue_index;\n",
    "  for (int i=0; i<eigenvalues.size(); i++) {\n",
    "    if (eigenvalues[i] == largest_eigenvalue) { // find the corresponding eigenvector's index\n",
    "      largest_eigenvalue_index = i;\n",
    "      break;\n",
    "    }\n",
    "  }\n",
    "  Eigen::VectorXd principal_component = eigenvectors.col(largest_eigenvalue_index).real(); \n",
    "  return principal_component;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  using std::chrono::high_resolution_clock;\n",
    "  using std::chrono::duration_cast;\n",
    "  using std::chrono::duration;\n",
    "  using std::chrono::milliseconds;\n",
    "\n",
    "  MatrixXd m = load_csv<MatrixXd>(\"massivedataset.csv\");\n",
    "\n",
    "  auto t1 = high_resolution_clock::now();\n",
    "  Eigen::VectorXd output = PCA(m);\n",
    "  auto t2 = high_resolution_clock::now();\n",
    "  \n",
    "\n",
    "  /* Getting number of milliseconds as an integer. */\n",
    "  auto ms_int = duration_cast<milliseconds>(t2 - t1);\n",
    "\n",
    "  /* Getting number of milliseconds as a double. */\n",
    "  duration<double, std::milli> ms_double = t2 - t1;\n",
    "  std::cout << \"time taken is\\n\";\n",
    "  // std::cout << ms_int.count() << \"ms\\n\";\n",
    "  std::cout << ms_double.count() << \"ms\\n\";\n",
    "  std::cout << \"answer is \\n\";\n",
    "  std::cout  << output  << std::endl;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 28.00154685974121 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\hrshv\\Documents\\CS-1390-Intro-to-Machine-Learning\\massivedataset.csv\")\n",
    "# pca = PCA()\n",
    "# a = pca.fit(data)\n",
    "# type(a)\n",
    "\n",
    " # apply PCA to the input features X\n",
    "st = time.time()\n",
    "pca = PCA(n_components=1)\n",
    "output = pca.fit_transform(data.T)\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time * 1000, 'ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn function takes around 100 ms in most cases.\n",
    "\n",
    "My function takes around 1000 ms, \n",
    "\n",
    "so the sklearn function is faster\n",
    "\n",
    "I am not sure but I think this could be, to a large extent, because operations with the pandas dataframe are faster than with an 'eigen' matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "X,y = fetch_openml(\n",
    "    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n",
    ")\n",
    "\n",
    "data = pd.DataFrame(X)\n",
    "target = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30000\n",
    "random_indices = np.random.choice(data.index, size=sample_size, replace=False)\n",
    "sample_data = data.loc[random_indices]\n",
    "sample_target = target.loc[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# I did not write the function below. I found an efficient function here:\n",
    "# https://stackoverflow.com/questions/34047540/python-clustering-purity-metric\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=1)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_iter': 100, 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'tol': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "n_clusters = 10\n",
    "\n",
    "# Create KMeans estimator\n",
    "kmeans = KMeans(init='k-means++', n_init=n_clusters, verbose=0, \n",
    "                random_state=None, copy_x=True, algorithm='lloyd')\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(kmeans, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV object to the data\n",
    "grid_search.fit(sample_data)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5879666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "\n",
    "# Create a KMeans instance with 10 clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=n_clusters, \n",
    "                max_iter=100, tol=0.0001, verbose=0, random_state=None, \n",
    "                copy_x=True, algorithm='lloyd')\n",
    "\n",
    "# Fit the KMeans model to the sample_data DataFrame\n",
    "kmeans.fit(sample_data)\n",
    "\n",
    "# Get the cluster labels for each datapoint in the sample_data DataFrame\n",
    "kmeans_pred = kmeans.predict(sample_data)\n",
    "purity_score(sample_target, kmeans_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean', \n",
    "#                 metric_params=None, algorithm='auto', leaf_size=30, \n",
    "#                 p=None, n_jobs=None)\n",
    "\n",
    "# dbscan.fit(sample_data)\n",
    "\n",
    "# dbscan_pred = dbscan.fit_predict(sample_data)\n",
    "\n",
    "# purity_score(sample_target, dbscan_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not able to fix the prediction with dbscan, even after changing the different hyperparameters a lot and trying with GridsearchCV\n",
    "\n",
    "From what I see here:\n",
    "\n",
    "https://crunchingthedata.com/when-to-use-dbscan/\n",
    "\n",
    "the algorithm the algorithm is not suited to solving problems with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meanshift = MeanShift(bandwidth=None, seeds=None, bin_seeding=False, \n",
    "#                       min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=100)\n",
    "\n",
    "# meanshift.fit(sample_data)\n",
    "\n",
    "# meanshift_pred = meanshift.fit_predict(sample_data)\n",
    "\n",
    "# purity_score(sample_target, meanshift_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a few things with meanshift, but in this case the main issue is that it is not running fast enough with a significant number of samples. Testing different hyperparameters with >= 1000 samples was effectively impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_vals = np.unique(y)\n",
    "\n",
    "# # Use count_nonzero() to count the occurrence of each unique value in the array\n",
    "# for val in unique_vals:\n",
    "#     count = np.count_nonzero(y == val)\n",
    "#     print(f\"{val}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
